{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import time\n",
    "from time import localtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(database=\"?\", user=\"?\", password=\"?\", host=\"?\", port=\"?\")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voucher price add\n",
    "tablename = '?'\n",
    "query = f'''\n",
    "    create table if not exists {tablename} (\n",
    "        id serial primary key,\n",
    "        brand text,\n",
    "        category text,\n",
    "        sub_category text,\n",
    "        product text,\n",
    "        product_id text,\n",
    "        product_sku text,\n",
    "        img text,\n",
    "        final_price text,\n",
    "        regular_price text,\n",
    "        disc_perc text,\n",
    "        voucher text,\n",
    "        voucher_price text,\n",
    "        installment text,\n",
    "        review text,\n",
    "        rating text,\n",
    "        product_link text,\n",
    "        batch text\n",
    "    );\n",
    "'''\n",
    "cur.execute(query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_insert(cat, j, articles, k, cur, conn, tablename):\n",
    "\n",
    "    try:\n",
    "        brand = articles[k]['data-brand'].strip().replace(\"'\", \"\")\n",
    "        category = cat[j][0]\n",
    "        sub_category = articles[k]['data-category'].strip().replace(\"'\", \"\")\n",
    "        product = articles[k]['data-title'].strip().replace(\"'\", \"\")\n",
    "        product_id = articles[k]['data-seller-product-id'].strip()\n",
    "        product_sku = articles[k]['product-sku'].strip()\n",
    "        img = articles[k].img['src'].strip()\n",
    "        final_price = articles[k].find_all('span', {'class': \"final-price\"})[0].text.strip().replace(\"đ\", \"\").replace(\".\", \"\").split(' ')[0]\n",
    "        regular_price = final_price if articles[k].find_all('span', {'class': \"price-regular\"}) == [] else articles[k].find_all('span', {'class': \"price-regular\"})[0].text.strip().replace(\"đ\", \"\").replace(\".\", \"\").split(' ')[0]\n",
    "        disc_perc = \"0%\" if articles[k].find_all('span', {'class': \"sale-tag sale-tag-square\"}) == [] else articles[k].find_all('span', {'class': \"sale-tag sale-tag-square\"})[0].text.strip().split(' ')[0]\n",
    "        voucher = [\"?\" if articles[k].find_all('p', {'class': \"price-tag\"}) == [] else articles[k].find_all('span', {'class': \"code\"})[0].text][0]\n",
    "        voucher_price = [\"?\" if articles[k].find_all('p', {'class': \"price-tag\"}) == [] else articles[k].find_all('span', {'class': \"price\"})[0].find('span').text.replace(\"đ\", \"\").replace(\".\", \"\")][0]\n",
    "        installment = [\"?\" if articles[k].find_all('span', {'class': \"installment-price-v2\"}) == [] else articles[k].find_all('span', {'class': \"installment-price-v2\"})[0].text][0]\n",
    "        review = [articles[k].find_all('p', {'class': \"review\"})[0].text.strip('\\(\\)') if articles[k].find_all('p', {'class': \"review\"}) != [] else \"Chưa có nhận xét\"][0]\n",
    "        rating = [\"?\" if articles[k].find_all('span', {'class': \"rating-content\"}) == [] else articles[k].find_all('span', {'class': \"rating-content\"})[0].find('span')['style'].split(':')[1]][0]\n",
    "        product_link = 'https://tiki.vn' + articles[k].a['href']\n",
    "        batch = strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n",
    "\n",
    "        query = f\"\"\"insert into {tablename} (brand, category, sub_category, product, product_id, product_sku, img, final_price, regular_price, disc_perc, voucher, voucher_price, installment, review, rating, product_link, batch)\n",
    "            values('{brand}', '{category}', '{sub_category}', '{product}', '{product_id}', '{product_sku}', '{img}', '{final_price}', '{regular_price}', '{disc_perc}', \n",
    "                '{voucher}', '{voucher_price}', '{installment}', '{review}', '{rating}', '{product_link}', '{batch}'\n",
    "            );\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup = get_web('https://tiki.vn')\n",
    "categories = soup.findAll('a', {'class', \"MenuItem__MenuLink-sc-181aa19-1 fKvTQu\"})\n",
    "category, link = [], []\n",
    "\n",
    "for h in range(len(categories)):\n",
    "    try:\n",
    "        link.append(categories[h]['href'])\n",
    "        category.append(categories[h].text)\n",
    "    except:\n",
    "        print('pass')\n",
    "        \n",
    "cat = list(zip(category,link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in range(len(cat)):\n",
    "    try:\n",
    "        soup = get_web(cat[j][1])\n",
    "        articles = soup.find_all('div', {'class': \"product-item\"})\n",
    "        print('Looping: ' + cat[j][1])\n",
    "        for k in range(len(articles)):\n",
    "            crawl_insert(cat, j, articles, k, cur, conn, tablename)\n",
    "  \n",
    "        links = soup.find_all('div', {'class': \"list-pager\"})\n",
    "        \n",
    "        # Exit while loop when reaching to the last page\n",
    "        while links[0].find_all('a', {'class': \"next\"}) != []:\n",
    "            try:\n",
    "                soup = get_web('https://tiki.vn' + links[0].find_all('a', {'class': \"next\"})[0]['href'])\n",
    "                articles = soup.find_all('div', {'class': \"product-item\"})\n",
    "                print('Looping: ', cat[j][0], links[0].find_all('a', {'class': \"next\"})[0]['href'].split('&')[1], sep=' ')\n",
    "                for i in range(len(articles)):\n",
    "                    crawl_insert(cat, j, articles, i, cur, conn, tablename)\n",
    "                links = soup.find_all('div', {'class': \"list-pager\"})\n",
    "            except:\n",
    "                continue\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print('Crawl done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}